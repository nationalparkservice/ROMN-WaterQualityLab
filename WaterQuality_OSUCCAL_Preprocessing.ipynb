{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HJ_nLmy4q_fn"
   },
   "outputs": [],
   "source": [
    "# Created by Balakrishna Vagvala and Sherrill Kirk R\n",
    "# This code depends upon the packages and libraries listed in the next cell\n",
    "# Date of creation: 12/1/2020\n",
    "# Script purpose: this script processes the OSU CCAL Lab delivered data, compares with expected fields file and make ready to be uploaded to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use openpyxl to load excel work book, this will help in having our current data as new sheet in the workbook\n",
    "# we use shutil to copy a file from it's location to a desired location,\n",
    "# this will help in copy pasting our original file to make changes on it\n",
    "# We use difflib library and import sequencematcher function, this will help us in comparing strings and give us a match ratio\n",
    "# We use numpy library and import it as np to make numerical operations for our program\n",
    "# we use pandas library and immport it as np to make data operations for our lab data\n",
    "# we use time library to import curent time of our function\n",
    "# we use OS library for different functions such as writing data to file and saving files\n",
    "# We use glob library and its glob function finds all the pathnames matching a specified pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfqegHbBgVt_",
    "outputId": "bff686fa-f7d8-49d8-90ae-0b21c330ea3d"
   },
   "outputs": [],
   "source": [
    "#importing pandas for data operations, numpy for numerical operations. time is used to have present time.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "#sequence matcher is used to compare strings and openpyxl is used to write workbooks.\n",
    "from difflib import SequenceMatcher\n",
    "from openpyxl import load_workbook\n",
    "from re import search\n",
    "from shutil import copyfile\n",
    "#importing reading functions and declaring the location of our file.\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhmE5T3u7VxB",
    "outputId": "52bd2f76-e443-4c41-c3d3-268ecd9f0dce"
   },
   "outputs": [],
   "source": [
    "# mounting google drive for google collab, not needed for native IDE's\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#Variables to be defined\n",
    "########################\n",
    "\n",
    "#file to check the field names and change if needed\n",
    "crosswalkpath = r\"D:\\Bala\\OSU CCAL\\osuccal_expectedfields.xlsx\"\n",
    "\n",
    "# enter the output path name\n",
    "currentoutputpath= r\"D:\\Bala\\OSU CCAL\\combined_output\\\\\"\n",
    "\n",
    "#path of the folder which contains all the test america lab files\n",
    "inputpath =r\"D:\\Bala\\OSU CCAL\\input\"\n",
    "\n",
    "#Wild Card Syntax used to define the files to be processed \n",
    "wildCardSyntax = \"**.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "#Output file name suffix for the Logfile and Subset/Appended file post processing\n",
    "outPutFileSuffix = \"OSUCCAL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2KIaL9VM7xac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRKO_032620.xlsx\n",
      "GRKO_042820.xlsx\n",
      "GRKO_071620.xlsx\n",
      "ROMN_112219_121719.xlsx\n"
     ]
    }
   ],
   "source": [
    "# runtime for file\n",
    "\n",
    "timestr = time.strftime(\"_%Y%m%d_%H%M\")\n",
    "runtimenow = timestr\n",
    "\n",
    "# the opening the log file\n",
    "logfile= open(currentoutputpath+outPutFileSuffix+runtimenow+\".txt\",\"w+\")\n",
    "\n",
    "\n",
    "listofdataframes = []\n",
    "\n",
    "#Path where all the excel files are residing to be processed\n",
    "all_files = glob.glob(os.path.join(inputpath, wildCardSyntax))\n",
    "\n",
    "\n",
    "for  no,f in enumerate(all_files):\n",
    "    df = pd.read_excel(f, sheet_name = 1)\n",
    "    #file to check crosswalks, i.e field names, this is an excel file's path\n",
    "    \n",
    "    #toread crosswalk file\n",
    "    crosswalk = pd.read_excel(crosswalkpath)\n",
    "    #file to process , this is an excel file's path\n",
    "#     data = \"D:\\Bala\\OSU CCAL\\input\\GRKO_032620_changed_names.xlsx\"\n",
    "\n",
    "    \n",
    "    \n",
    "    filename= os.path.basename(f)\n",
    "    print(filename)\n",
    "    sep = '.'\n",
    "    filename = filename.split(sep, 1)[0]\n",
    "\n",
    "    \n",
    "\n",
    "    firstline=[]\n",
    "    firstline.append(\"File Being Processed is: \" + filename+ \"\\n \\n \\n#########\\n\\n\" )\n",
    "    #toread the lab file, we mention sheet name of excel file in the second argument, here GRKO Data is our sheet name in excel file by lab\n",
    "    # osudata = pd.read_excel(data, 'GRKO Data')\n",
    "    #to specify sheet number instead of sheet name\n",
    "#     osudata = pd.read_excel(data, sheet_name = 1)\n",
    "\n",
    "    #here we process our data so that we have just the field names and field values in our dataframe\n",
    "    osudata = df\n",
    "    df = df.iloc[2:]\n",
    "    df = df.rename(columns=df.iloc[0])\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.iloc[1:,1:]\n",
    "    \n",
    "\n",
    "    # to remove \\n and \\r from lab delivareables, they are influencing our data and output, so we are removing them\n",
    "\n",
    "    dfStrip = osudata.replace('\\n',' ', regex=True)\n",
    "\n",
    "    dfStrip2 = dfStrip.replace('\\r','', regex=True)\n",
    "\n",
    "    df = dfStrip2\n",
    "\n",
    "    df = df.iloc[2:]\n",
    "\n",
    "    df = df.rename(columns=df.iloc[0])\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    df = df.iloc[1:,1:]\n",
    "\n",
    "    osufields = list(df.columns)\n",
    "    labfields = crosswalk['NativeDeliveryFIelds'].tolist()\n",
    "    expectedfields = crosswalk['CrossWalkedFields'].tolist()\n",
    "    # this cell is to check for coloumns and export irregularities into warning list\n",
    "    \n",
    "    matchingfields =[]\n",
    "    lenghtoffields = []\n",
    "    morethanninety = []\n",
    "    lessthanninety = []\n",
    "    newfields=[]\n",
    "    duplicatefields=[]\n",
    "    missedfields = []\n",
    "    p=''\n",
    "    q=''\n",
    "    x=''\n",
    "    if len(osufields)>len(labfields):\n",
    "        lenghtoffields.append('\\n\\nThis '+filename+' file has new columns.\\n')\n",
    "\n",
    "    if len(osufields)<len(labfields):\n",
    "        lenghtoffields.append('This '+filename+' file has lesser columns than expected.\\n')\n",
    "\n",
    "    duplicatefields.append ('\\n\\n#####\\n\\nFields with duplicate field names \\n') \n",
    "    missedfields.append ('\\n\\n######\\n\\nFields which are expected, but not in our data\\n')\n",
    "    matchingfields.append ('\\n\\n#####\\n\\nFields that matched as expected') \n",
    "    morethanninety.append ('\\n\\n#####\\n\\nFields without 100% match but had more than 90% Match\\n') \n",
    "    lessthanninety.append ('\\n\\n#####\\n\\nFields with less than 90% match but had more than 80% Match\\n') \n",
    "    newfields.append ('\\n\\n#####\\n\\nNew Fields , these are not found in crosswalk and have been removed from data \\n') \n",
    "\n",
    "    for i in osufields:\n",
    "        if i == 'Date': \n",
    "            x = osufields.index(i)\n",
    "            osufields[x] = osufields[x-1] +(\" \") +osufields[x]\n",
    "            continue \n",
    "    for j in labfields:\n",
    "        if j == 'Date':\n",
    "            x = labfields.index(j)\n",
    "            labfields[x] = labfields[x-1] +(\" \") + labfields[x]\n",
    "    df.columns = osufields\n",
    "\n",
    "    for i in expectedfields: \n",
    "        for j in osufields:\n",
    "            k = i+\".\"+\"1\"\n",
    "            if search(k, j):\n",
    "                print(k)\n",
    "                duplicatefields.append(\"Warning: \"+i+ \" has a duplicate field \\n\")\n",
    "                break\n",
    "\n",
    "    for i in osufields:     \n",
    "        if i in labfields:\n",
    "            matchingfields.append(\"column: \"+ i +\"  has sucessfully matched \\n\")\n",
    "            x = osufields.index(i)\n",
    "            y = labfields.index(i)        \n",
    "            osufields[x] = expectedfields[y]\n",
    "            continue      \n",
    "\n",
    "        else:\n",
    "            result = \"\"\n",
    "            count = 0\n",
    "            ratio = 0  \n",
    "\n",
    "            for words in labfields:\n",
    "                ratio = SequenceMatcher(None,i,words).ratio()\n",
    "                if ratio > count:\n",
    "                    count = ratio\n",
    "                    result =words         \n",
    "\n",
    "            if count > 0.9:\n",
    "                x = osufields.index(i)\n",
    "                y = labfields.index(result)        \n",
    "                morethanninety.append(\"Warning: \"+osufields[x]+\" was crosswalk to \"+ result + \" has matched with more than 90% accuracy \\n\")\n",
    "                osufields[x] = expectedfields[y]\n",
    "\n",
    "\n",
    "\n",
    "            elif count> 0.8:\n",
    "                x = osufields.index(i)\n",
    "                y = labfields.index(result)\n",
    "                lessthanninety.append(\"Warning: \"+osufields[x]+\" was crosswalk to \"+ result + \" has matched with more than 80% accuracy  \\n\")\n",
    "\n",
    "                osufields[x] = expectedfields[y]\n",
    "\n",
    "\n",
    "\n",
    "    # temp to check the missing columns in our processed lab deliverable \n",
    "    missedfields = []\n",
    "    missedfields.append ('\\n\\n######\\n\\nFields which are expected, but not in our data\\n')\n",
    "    temp =  expectedfields.copy()\n",
    "    for i in osufields:\n",
    "        if i in temp:\n",
    "            temp.remove(i) \n",
    "    for i in temp:\n",
    "        missedfields.append(\"column: \"+ i + \" is expected but is not in our current data \\n \")        \n",
    "\n",
    "    df.columns = osufields\n",
    "\n",
    "    for i in osufields:\n",
    "        if i not in expectedfields:\n",
    "            newfields.append(\"Warning: \"+i+\" is a new field, look into it\\n\")\n",
    "            x = list(df.columns).index(i)\n",
    "            del osufields[x]\n",
    "            cols = [i for i in range(df.shape[1])]\n",
    "            cols.remove(x)\n",
    "            df = df.iloc[:,cols]\n",
    "\n",
    "\n",
    "    df.columns = osufields\n",
    "    # to reindex columns based on expected fields\n",
    "    \n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    \n",
    "    df = df.reindex(expectedfields, axis=1)\n",
    "    # this will drop columns with no values which are picked up from re-indexing\n",
    "    df = df.dropna(axis=1)\n",
    "\n",
    "    worked_files = firstline  + lenghtoffields+ morethanninety +lessthanninety + duplicatefields + missedfields+newfields + matchingfields\n",
    "    for i in range(len(worked_files)):\n",
    "        logfile.write(worked_files[i])\n",
    "        \n",
    "    count_row = df.shape[0]  # Gives number of rows\n",
    "    count_col = df.shape[1]  # Gives number of columns\n",
    "\n",
    "    logfile.write('\\n\\n######\\nNumber of Rows = '+ str(count_row)+'\\nNumber of Columns = '+ str(count_col)+'\\nthis is end of the Log File for '+filename+'\\n\\n###########\\n\\n')\n",
    "          \n",
    "\n",
    "#     pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#     df\n",
    "\n",
    "    #saving our new csv file into a downloadable format. and it can be saved\n",
    "    # df.to_csv( \"data_fieldsprocessed\"+timestr+\".csv\", index=False, encoding='utf-8-sig')\n",
    "    #if you want an excel output\n",
    "    # f = open\n",
    "    # df.to_excel(\"pre-processed\"+timestr+\".xls\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # to make a duplicate of our file and export our processed \n",
    "    # copyfile ('D:\\Bala\\OSU CCAL\\input\\GRKO_032620.xlsx' , './processed'+filename+'.xlsx')\n",
    "    processed_path = (currentoutputpath+\"\\\\\"+filename+'_processed.xlsx')\n",
    "    copyfile (f , processed_path)\n",
    "#     data\n",
    "\n",
    "    # rename our copied file and place it at the location we want\n",
    "\n",
    "    # os.rename(r'D:\\Bala\\OSU CCAL\\output\\processed.xlsx', r'D:\\Bala\\OSU CCAL\\output\\OSUCCAL_processed_' + filename + '.xlsx')\n",
    "\n",
    "    # enter the path of the processed named copy of the original file\n",
    "    # processed_path = 'D:\\Bala\\OSU CCAL\\output\\OSUCCAL_processed__filename.xlsx'\n",
    "    # to load our data frame into excel sheet\n",
    "    book = load_workbook(processed_path)\n",
    "    writer = pd.ExcelWriter(processed_path, engine = 'openpyxl')\n",
    "    writer.book = book\n",
    "    # enter the desired sheet name below\n",
    "    sheetname = 'preprocessed' #enter your sheet name here in the space, we can have it as a prefix as you want\n",
    "    df.to_excel(writer, sheet_name = sheetname)\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    listofdataframes.append(df)\n",
    "\n",
    "logfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file = pd.concat( listofdataframes) \n",
    "\n",
    "combined_file.to_csv( currentoutputpath+\"combined_csv of \"+outPutFileSuffix+timestr+\".csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read Excel file of the crosswalk\n",
    "# data_xls = pd.read_excel(crosswalk,dtype=str, index_col=None)\n",
    "# checkfile = data_xls\n",
    "#sheet to speicy sheet number and index_col to have a default index or not.\n",
    "#data_xls.to_csv('csvfile.csv', encoding='utf-8', index=False)\n",
    "\n",
    "#     uncomment below line if its csv file, remove the '#' symbol\n",
    "# all_files = glob.glob(os.path.join(path, \"*TalStandard.csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OSU CCAL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
