{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by Balakrishna Vagvala and Sherrill Kirk\n",
    "# This code depends upon the packages and libraries listed in the next cell\n",
    "# Date of creation: 12/1/2020\n",
    "# Script purpose: this script processes the Test America Lab delivered data, compares with expected fields file and make ready to be uploaded to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use openpyxl to load excel work book, this will help in having our current data as new sheet in the workbook\n",
    "# we use shutil to copy a file from it's location to a desired location,\n",
    "# this will help in copy pasting our original file to make changes on it\n",
    "# We use difflib library and import sequencematcher function, this will help us in comparing strings and give us a match ratio\n",
    "# We use numpy library and import it as np to make numerical operations for our program\n",
    "# we use pandas library and immport it as np to make data operations for our lab data\n",
    "# we use time library to import curent time of our function\n",
    "# we use OS library for different functions such as writing data to file and saving files\n",
    "# We use glob library and its glob function finds all the pathnames matching a specified pattern\n",
    "# We use xlsx writer to write sheets and reed sheets into an excel file, to install for the first time, uncomment the below line\n",
    "#!pip install XlsxWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pandas for data operations, numpy for numerical operations. time is used to have present time.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "#sequence matcher is used to compare strings and openpyxl is used to write workbooks.\n",
    "from difflib import SequenceMatcher\n",
    "from openpyxl import load_workbook\n",
    "from shutil import copyfile\n",
    "    \n",
    "from re import search\n",
    "#importing reading functions and declaring the location of our file.\n",
    "import os, glob\n",
    "import os.path\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#Variables to be defined\n",
    "########################\n",
    "\n",
    "#file to check the field names and change if needed\n",
    "crosswalkpath = r\"D:\\ROMN\\working\\Misc\\4People\\4Bala\\WaterQualityProcessing\\TestAmerica\\TestAmerica_Delivery_ExpectedFields.xlsx\"\n",
    "\n",
    "#enter the output path name\n",
    "currentoutputpath= r\"D:\\ROMN\\working\\Misc\\4People\\4Bala\\WaterQualityProcessing\\TestAmerica\\2020Test\\Preprocessed\"\n",
    "\n",
    "#path of the folder which contains all the test america lab files\n",
    "path =r\"D:\\ROMN\\working\\Misc\\4People\\4Bala\\WaterQualityProcessing\\TestAmerica\\2020Test\"\n",
    "\n",
    "#Wild Card Syntax used to define the files to be processed \n",
    "wildCardSyntax = \"*TalStandard*.csv\"\n",
    "\n",
    "#Output file name suffix for the Logfile and Subset/Appended file post processing\n",
    "outPutFileSuffix = \"TestAmericaAll_2020\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crosswalk = pd.read_excel(crosswalkpath)\n",
    "checkfile = crosswalk\n",
    "\n",
    "#Path where all the excel files are residing to be processed\n",
    "all_files = glob.glob(os.path.join(path, wildCardSyntax))\n",
    "\n",
    "# this code block saves our files into a path we desire\n",
    "processedsavefiles = currentoutputpath\n",
    "processed_path = (currentoutputpath+'\\\\')\n",
    "\n",
    "# runtime for file\n",
    "\n",
    "timestr = time.strftime(\"_%Y%m%d_%H%M\")\n",
    "runtimenow = timestr\n",
    "\n",
    "# the opening the log file\n",
    "logfile = open(currentoutputpath+\"\\\\\" + outPutFileSuffix +runtimenow+\".txt\",\"w+\")\n",
    "\n",
    "\n",
    "listofdataframes = []\n",
    "listofunchangeddataframes =[]\n",
    "\n",
    "for  no,f in enumerate(all_files):\n",
    "    \n",
    "    #df = pd.read_excel(f)\n",
    "    df = pd.read_csv(f)\n",
    "#     uncomment below line if its csv file, remove the '#' symbol\n",
    "#    df = pd.read_csv(f)\n",
    "    listofunchangeddataframes.append(df)\n",
    "    \n",
    "\n",
    "    \n",
    "    #file to process , this is an excel file's path\n",
    "    filename= os.path.basename(f)\n",
    "    print(filename)\n",
    "    sep = '.'\n",
    "    filename = filename.split(sep, 1)[0]\n",
    "\n",
    "    \n",
    "\n",
    "    firstline=[]\n",
    "    firstline.append(\"File Being Processed is: \" + filename )\n",
    "    #toread the lab file, we mention sheet name of excel file in the second argument, here GRKO Data is our sheet name in excel file by lab\n",
    "\n",
    "    testamfields = list(df.columns)\n",
    "    expected_fields = checkfile['ExpectedFields'].tolist() \n",
    "    \n",
    "    matchingfields =[]\n",
    "    lenghtoffields = []\n",
    "    morethanninety = []\n",
    "    lessthanninety = []\n",
    "    newfields=[]\n",
    "    duplicatefields=[]\n",
    "    missedfields = []\n",
    "    x=''\n",
    "    if len(testamfields)>len(expected_fields):\n",
    "        lenghtoffields.append('\\n\\n#########File: '+filename+' file has new columns.\\n')\n",
    "\n",
    "    if len(testamfields)<len(expected_fields):\n",
    "        lenghtoffields.append('\\n\\n#########File: '+filename+' file has less columns than expected.\\n')\n",
    "\n",
    "    duplicatefields.append ('\\n\\n#####\\n\\nFields with duplicate field names:\\n') \n",
    "    missedfields.append ('\\n\\n######\\n\\nFields which are expected, but not in our data:\\n')\n",
    "    matchingfields.append ('\\n\\n#####\\n\\nFields that matched as expected:\\n') \n",
    "    morethanninety.append ('\\n\\n#####\\n\\nFields without 100% match but had more than 90% Match:\\n') \n",
    "    lessthanninety.append ('\\n\\n#####\\n\\nFields with less than 90% match but had more than 80% Match:\\n') \n",
    "    newfields.append ('\\n\\n#####\\n\\nNew Fields, these are not found in crosswalk and have been removed from data:\\n') \n",
    "\n",
    "    \n",
    "    for i in expected_fields: \n",
    "        for j in testamfields:\n",
    "            k = i+\".\"+\"1\"\n",
    "            if search(k, j):\n",
    "                print(k)\n",
    "                duplicatefields.append(\"Warning: \"+i+ \" has a duplicate field \\n\")\n",
    "                break\n",
    "    \n",
    "#     for i in testamfields:   \n",
    "#         for j in expected_fields:\n",
    "#             if j in i:\n",
    "#                 if j != i:\n",
    "#                     ratio = SequenceMatcher(None,i,j).ratio()\n",
    "#                     if ratio != 1:\n",
    "#                         duplicatefields.append(\"Warning: \"+i+ \" is a duplicate field, almost matched with \"+j+\" \\n\")                    \n",
    "#                     x = testamfields.index(i)\n",
    "#                     cols = [i for i in range(df.shape[1])]\n",
    "#                     cols.remove(x)\n",
    "#                     cols.remove(x+1)\n",
    "#                     df = df.iloc[:,cols]\n",
    "#                     del testamfields[x]\n",
    "#                     del testamfields[x]\n",
    "\n",
    "    for i in testamfields:     \n",
    "        if i in expected_fields:\n",
    "            matchingfields.append(\"column: \"+ i +\"  has sucessfully matched \\n\")\n",
    "            continue      \n",
    "\n",
    "        else:\n",
    "            result = \"\"\n",
    "            count = 0\n",
    "            ratio = 0  \n",
    "\n",
    "            for words in expected_fields:\n",
    "                ratio = SequenceMatcher(None,i,words).ratio()\n",
    "                if ratio > count:\n",
    "                    count = ratio\n",
    "                    result =words         \n",
    "\n",
    "            if count > 0.9:\n",
    "                x = testamfields.index(i)\n",
    "                y = expected_fields.index(result)        \n",
    "                morethanninety.append(\"Warning: \"+testamfields[x]+\" was crosswalk to \"+ result + \" has matched with more than 90% accuracy \\n\")\n",
    "                testamfields[x] = expected_fields[y]\n",
    "                \n",
    "            elif count> 0.8:\n",
    "                x = testamfields.index(i)\n",
    "                y = expected_fields.index(result)        \n",
    "                morethanninety.append(\"Warning: \"+testamfields[x]+\" was crosswalk to \"+ result + \" has matched with more than 90% accuracy \\n\")\n",
    "                testamfields[x] = expected_fields[y]\n",
    "\n",
    "\n",
    "\n",
    "    # temp to check the missing columns in our processed lab deliverable \n",
    "    missedfields = []\n",
    "    missedfields.append ('\\n\\n######\\n\\nFields which are expected, but not in our data:\\n')\n",
    "    temp =  expected_fields.copy()\n",
    "    for i in testamfields:\n",
    "        if i in temp:\n",
    "            temp.remove(i) \n",
    "    for i in temp:\n",
    "        missedfields.append(\"column: \"+ i + \" is expected but is not in our current data\\n \")        \n",
    "\n",
    "    df.columns = testamfields\n",
    "\n",
    "    for i in testamfields:\n",
    "        if i not in expected_fields:\n",
    "            newfields.append(\"Warning: \"+i+\" is a new field, look into it\\n\")\n",
    "            x = list(df.columns).index(i)\n",
    "            del testamfields[x]\n",
    "            cols = [i for i in range(df.shape[1])]\n",
    "            cols.remove(x)\n",
    "            df = df.iloc[:,cols]\n",
    "\n",
    "\n",
    "    df.columns = testamfields\n",
    "    # to reindex columns based on expected fields\n",
    "\n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "    df = df.reindex(expected_fields, axis=1)\n",
    "    # this will drop columns with no values which are picked up from re-indexing\n",
    "#     df = df.dropna(axis=1)\n",
    "\n",
    "    worked_files = firstline  + lenghtoffields+ morethanninety +lessthanninety + duplicatefields + missedfields+newfields + matchingfields\n",
    "    for i in range(len(worked_files)):\n",
    "        logfile.write(worked_files[i])\n",
    "\n",
    "    logfile.write('\\n\\n######\\nthis is end of the Log File for '+filename+'\\n\\n###########\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "          \n",
    "\n",
    "#     pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#     df\n",
    "\n",
    "    #saving our new csv file into a downloadable format. and it can be saved\n",
    "    # df.to_csv( \"data_fieldsprocessed\"+timestr+\".csv\", index=False, encoding='utf-8-sig')\n",
    "    #if you want an excel output\n",
    "    # f = open\n",
    "    # df.to_excel(\"pre-processed\"+timestr+\".xls\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # to make a duplicate of our file and export our processed \n",
    "    # copyfile ('D:\\Bala\\Testamerica\\Testamerica_practice\\Test america Practice data 28-11\\280-136365-1_TalStandard.csv\\' , './processed'+filename+'.xlsx')\n",
    "\n",
    "#     data\n",
    "\n",
    "    # rename our copied file and place it at the location we want\n",
    "\n",
    "    # os.rename(r'D:\\Bala\\Testamerica\\output\\processed.xlsx', r'D:\\Bala\\Testamerica\\output\\Testamerica_processed_' + filename + '.xlsx')\n",
    "\n",
    "    #Replacing Nulls with missing values for easae of operation for the sample type desc coloumn.\n",
    "    df['Sample Type Desc'] = df['Sample Type Desc'].fillna('Missing value')\n",
    "    \n",
    "    #removing description types of Lab control sample, Lab control sample duplicate and method blank from Sample type description of Test america data\n",
    "    df_new = df.loc[(df['Sample Type Desc'] != 'Lab Control Sample') & (df['Sample Type Desc'] != 'Lab Control Sample Duplicate')& (df['Sample Type Desc'] != 'Method Blank' )]\n",
    "    #df_new is the subset data set.\n",
    "    df = df_new\n",
    "#     df = df_new.replace('Missing value', '')\n",
    "    df.loc[df['Sample Type Desc'] == 'Missing value', 'Sample Type Desc'] = ''\n",
    "    df_new = df\n",
    "    listofdataframes.append(df_new)\n",
    "    \n",
    "logfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = listofdataframes[1]\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "count = 0\n",
    "\n",
    "for  no,f in enumerate(all_files):\n",
    "    for i in range(len(listofdataframes)):          \n",
    "        if i == count:\n",
    "            count = count + 1\n",
    "            filename= os.path.basename(f)\n",
    "            print(filename)\n",
    "            sep = '.'\n",
    "            filename = filename.split(sep, 1)[0]\n",
    "\n",
    "            processed_path = (processedsavefiles+ \"\\\\\" + filename + '_processed.xlsx')\n",
    "\n",
    "            #read_file = pd.read_excel(f)\n",
    "            read_file = pd.read_csv(f)\n",
    "    #         read_file = pd.read_csv (f)\n",
    "            read_file.to_excel (processed_path,sheet_name = 'OriginalData', index = None, header=True)\n",
    "\n",
    "            book = load_workbook(processed_path)\n",
    "            writer = pd.ExcelWriter(processed_path, engine = 'openpyxl')\n",
    "            writer.book = book\n",
    "            # enter the desired sheet name below\n",
    "            sheetname = 'preprocessed' #enter your sheet name here in the space, we can have it as a prefix as you want\n",
    "            z = listofdataframes[i]\n",
    "            z.to_excel(writer, sheet_name = sheetname, index = None)\n",
    "\n",
    "            writer.save()\n",
    "            writer.close()\n",
    "            break\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izl_4fVdHK8w"
   },
   "outputs": [],
   "source": [
    "#combining all csv files into a single file\n",
    "combined_csv = pd.concat( listofdataframes, axis=0, ignore_index=True)\n",
    "df = combined_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcDH2EUMg58h"
   },
   "outputs": [],
   "source": [
    "#checking our subset data numbers\n",
    "df['Sample Type Desc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qD346rsiiyI"
   },
   "outputs": [],
   "source": [
    "#to download our subset data and rename our subset to df.\n",
    "df.to_csv( currentoutputpath + \"\\\\\" + outPutFileSuffix + timestr+\".csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "testamerica",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
